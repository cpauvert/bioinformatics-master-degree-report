---
title: "Results"
author: Charlie PAUVERT
classoption: a4paper
fontsize: 12pt
lang: en
output:
  pdf_document:
    pandoc_args: [ "--latex-engine=/usr/bin/pdflatex" ] 
    toc: yes
    number_section: true
header-includes:
 - \usepackage{libertine}
--- 

# Results (22p)

> "Which micro-organisms exist in the studied ecosystem?"

We are trying to answer this question through the proxy of reference genomes and metagenomics reads alignments.
Hundreds of genomes extracted from dairy products are currently available in genomics databases [@almeida_construction_2014].
In the scope of Food-Microbiome projects, we rely on existing reference genomes to (1) identify micro-organisms present in the ecosystem --if possible to the *strain* level-- and (2) characterise low-abundant organisms.


In this chapter, I will focus on my two-year work around a metagenomics analysis tool --`GeDI`-- through four main axes.
I will present (1) several approaches explored to improve the tool, followed by (2) a comparison with similar tools and (3) how this module was integrated with others, to conclude with (4) results from its application on simulated and real datasets.


## Scientific and computational improvements of GeDI

Two kind of improvements are highlighted concerning this tool. 
First, scientific improvements through an exploration of micro-organisms taxonomical classification.
Second, computational improvements that covers  software performance as well as proper documentation.


```{r previous-approaches, engine="tikz",engine.opts=list(template="template-tikz2pdf.tex"),echo=FALSE, fig.align="center",fig.scap="Previous modeling approaches summary embedded in GeDI.", fig.cap="Previous modeling approaches summary embedded in GeDI. These different approaches were tested at different period depicted as symbols: before my internship ($\\circ$), during my first year of master degree internship ($\\diamond$) and during my two-year apprenticeship ($\\star$).",cache=TRUE}
\input{figures/fig-previous-approaches}
```

We want an automatic criterion to infer a micro-organism presence or absence from its alignment data.
To this end, we have focused on several modeling approaches --summarised in Figure \ref{fig:previous-approaches}-- throughout my work in BAC team.


###Modeling approaches

Our method is based on metagenomics reads alignment onto a set of reference genomes.
It is followed by an goodness of fit analysis between observed and predicted genome coverage.
The predicted coverage stem from a model assuming that the considered micro-organism genome exists in the ecosystem.

However, ecosystem genomes usually differ from corresponding genomes in databases.
Hence this constraint harden reads alignment analysis in metagenomics, and subsequent modeling efforts.
We need to be able (1) to identify a strain if its reference genome is available and (2) find the closest species when it is not the case.
We struggled to define a model that provide both accuracy and flexibility.
I have worked to explore, test and precise several modeling approaches. Approaches principles are briefly reviewed below.

####Previous modeling approaches

#####Homogeneity
Lander and Waterman stated that reads distribution is homogeneous on a genome as long as: (1) sequencing is a random process and (2) reads stem from the considered genome [see -@lander_genomic_1988].
This work is relevant in metagenomics for a sufficient reads number.
In this case, testing the reads distribution homogeneity is a proxy for testing presence or absence of a genome.
This hypothesis was tested by comparing observed and expected genome coverage --under the homogeneity hypothesis-- before and during my internship. 
However, thresholds used to compare distributions were empirical and  we could not explicitly validated this approach. Nevertheless, it yields interesting results as long as repeated regions were not too numerous and the reference genome was close to strains in the dataset.

#####Reads position intervals
I then explored several leads  in order to improve automatic taxonomic identification.
Simulated datasets enable reads distributions exploration and modeling.
Intervals between every two successive reads aligned is supposed to follow a geometric law.
This approach was meant to target genome with a low coverage, that is low abundant micro-organisms.
However, this model was too strict and could only be applied if reads were aligned to the same genome reads stem from.
For example, reads from a strain A aligned to the reference genome A would yield a reads interval distribution suitable to pass the test.
However, it was not the case if these same reads were aligned to a close strain of genome A --say $98\%$ identity.


#####Scores and ROC curves
In order to circumvent previously identified limits, I have also worked on threshold determination.
I relied on simulated datasets to infer decision rules regarding a micro-organism presence or absence from alignment data.
For instance reads from one strain were aligned to more or less distant genomes.
Simple datasets --like the latter-- yield interesting results from this approach.
However, it did not scales to metagenomics data where reads stem from multiples contributors.

---

Previous approaches were based on genome coverage. While bypassing potential annotation bias, these methods were impacted by both inter-species and intra-species genomics variation noises --e.g., pseudo-genes, mobile elements, etc.
In 2015, I started going back to a previous approach in the team: focused on CDS --_Coding DNA Sequences_.

####Exploratory approach to micro-organism identification

A CDS-centred approach has several advantages. First, these coding sequences are much more conserved. Then, CDS can be strain-specific and yield organism functions --inferred for instance by protein sequences prediction.
Moreover, reference genomes sheared in multiple contigs could also be used.
Minimal annotation of hypothetical CDS is automatically done nowadays.
Hence even draft genomes could provide information on the studied metagenome.

#####CDS coverage densities
We define a strain as a _reference_ here: _Streptococcus salivarius_ JIM8777.
Shotgun metagenomics reads are simulated from this reference genome and nine others genomes more or more distant to the reference.
These additional genomes can be break down into 3 closeness classes: close strains, sub-species and close species --described in figure \ref{fig:strep}.
The ten reads dataset are then _independently_ processed by our tool GeDI --see former version \ref{fig:former-gedi}.
Reads aligned in CDS that are not filtered are used to compute CDS coverage. These coverage are then normalised by CDS length and presented in the figure \ref{fig:distrib}.


```{r data, echo=F, cache=TRUE, message=FALSE, warnings=FALSE}
data_summary<-read.table("aggregation_summary.TN.corr.filt.txt",  h=T,sep="\t", quote="")
# Récupération des variables d'intérêt
norm_data<-data_summary[,c(2,4,5,8,9,11,19:25)]
# Normalisation par la taille des CDS
norm_data$NbReads_0.3mismatchs<-(norm_data$NbReads_0.3mismatchs/(norm_data$FragmentEnd - norm_data$FragmentStart))
norm_data$Normalisation<-TRUE 

# Label classement des génomes selon la proximité
norm_data$Prox<-vector(mode = "numeric", length = length(norm_data$SourceGenome))

norm_data$Prox[norm_data$SourceGenome %in% c("Streptococcus_salivarius_JIM8777")] <- 0
norm_data$Prox[norm_data$SourceGenome %in% c("Streptococcus_salivarius_CCHSS3", 
                                             "Streptococcus_salivarius_NCTC8618",
                                             "Streptococcus_salivarius_K12-ord-b")] <- 1
norm_data$Prox[norm_data$SourceGenome %in% c("Streptococcus_thermophilus_JIM_8232", 
                                             "Streptococcus_salivarius_PS4",
                                             "Streptococcus_vestibularis_ATCC_49124")] <- 2

norm_data$Prox[norm_data$SourceGenome %in% c("Streptococcus_agalactiae_NEM316",
                                        "Streptococcus_infantarius_subsp__infantarius_ATCC_BAA102")]<-3

norm_data$Prox[norm_data$SourceGenome %in% c("Streptococcus_mutans_UA159")]<-3

## Proportions de CDS non couverts en fonction de l'origine des reads
require(plyr, quietly=TRUE)
proportions <- ddply(norm_data, .(Prox), summarize, Prop = mean(NbReads_0.3mismatchs == 0))
proportions0 <- format(proportions[ , 2]*100, digits = 4)

## Data frame pour profils de couvertures triées
data_cov<-ddply(ddply(norm_data, .(SourceGenome,Prox) , summarize, NbReadsSort = sort(NbReads_0.3mismatchs)),
		~SourceGenome, mutate, Rank=order(NbReadsSort)/length(NbReadsSort))
#
require(ggplot2,quietly=T)
distZ<-ggplot(norm_data,aes(x=NbReads_0.3mismatchs , fill=as.factor(Prox))) + geom_density() + scale_fill_manual(values=c("#2aa2e0","#f47d20", "#8bac21", "#ed1a5c"),labels=c("0: 1 reference","1: 3 close strains","2: 3 sub-species","3: 3 close species"), guide= guide_legend(direction="vertical"))+ labs(title="(A) With unaligned CDS", x="Reads number per base in a CDS", y="Density",fill="Closeness class")  +  facet_grid(Prox~.)+theme_minimal()+ theme(strip.text.y=element_text(color="black", face="bold"),strip.background = element_rect(fill="#A9A9A9"),legend.position="bottom") 
dist<-ggplot(norm_data[which(norm_data$NbReads_0.3mismatchs != 0),],aes(x=NbReads_0.3mismatchs , fill=as.factor(Prox))) + geom_density() + scale_fill_manual(values=c("#2aa2e0","#f47d20", "#8bac21", "#ed1a5c"))+ labs(title="(B) Without unaligned CDS", x="Reads number per base \n in a CDS (>= 1 read)", y="Density",fill="Closeness class")  + geom_text(data=proportions,x = 0.12,y=50, aes(label=paste("rho[",Prox,"]", sep="")),parse = T)+ geom_text(data=proportions,x = 0.15,y=50, aes(label=paste("= ",round(Prop*100)," %", sep="")))+  facet_grid(Prox~.)+theme_minimal()+ theme(strip.text.y=element_text(color="black", face="bold"),strip.background = element_rect(fill="#A9A9A9"),legend.position="none") 
```


```{r distrib, echo=F, fig.align="center", fig.scap="Training dataset CDS coverage densities depending on closeness classes.", fig.cap="Training dataset CDS coverage densities depending on closeness classes. Reads counts are normalised by each CDS length of the reference \\textit{Streptococcus salivarius} JIM8777. (A) Densities of these values are presented on the left pane. (B) CDS without any reads aligned are excluded from densities estimation on the right pane. Densities are showed by closeness class. Hence, the first one only includes one genome --the reference strain--, others classes encompass three genomes each. Unaligned CDS ratios are showed by closeness class despite being excluded from densities. This ratio is noted $\\rho_c$ for a class $c$ where $c\\in [0;3]$ and is a parameter in the mixture model.", cache=FALSE}
require(gridExtra, quietly=T)
grid.arrange(distZ, dist, ncol=2,widths=c(0.75,1))
```

In the left pane of the figure \ref{fig:distrib}, the first density --in blue-- represents how CDS coverage values are distributed when the same genome --the exact strain-- is used both for simulating reads and aligning them on this reference genome.
The latter CDS coverage density follows a Gaussian distribution with a mean $\mu$ and variance $\sigma^2$. Therefore, a majority of CDS are covered around the mean --here around 0.04 reads per base in a CDS.
On both sides of this bell curve stands CDS that are either much more covered than the mean coverage or very less covered. Their low density indicate that they are few CDS well covered and few CDS highly covered.
No CDS are left uncovered when enough reads are aligned on the exact reference.
Hence, there is no difference between the blue curves on pane (A) and (B) in figure \ref{fig:distrib}.

The second curve represents CDS coverage yields by aligning the previous dataset on a close strain.
This curve is slightly shifted to the left, indicating a lesser mean coverage.
But most notably, some CDS of the reference genome are not covered at all by reads from a close strain --it is indicated by the bump on the left side of the plot.
Interestingly, once null coverages are removed --see the right pane of the figure \ref{fig:distrib}-- the latter density looks like the exact strain coverage density.
In general, the less close the genome, the more CDS uncovered, shifting densities to the left. Thus, density of closeness class 4 looks more like an exponential rather than a Gaussian.
The last density --closeness class $3$-- possess a representative density, breaking with Gaussian like densities previously. Only a few CDS are covered, hence a important quantity of null values shape this density.
The few CDS covered matches housekeeping genes covered by a few reads.


#####CDS coverage densities crystallise closeness information
CDS coverage densities seems to be a good proxy for the closeness between a reference genome and reads aligned.
They appear (1) to be sensitive enough so that close strains reads aligned to the reference yields both a Gaussian density --see closeness class $0$ and $1$-- and (2) to be specific enough to express distinct signal between closeness class through the $\rho_c$ parameter.
The unique shape of the more distant class --closeness class $3$-- density ensure to model noisy contributions to the alignment such as reds from any genomes.  This non specific noise signal will be used to feature reference genomes absent from the ecosystem.




#####A mixture model to unravel contributors genomes
Among available reference genomes, we want to be able to distinguish (1) exact strains that yields the metagenomics reads --if available, (2) close strains or species, and (3) absent genomes.

Metagenomics sequencing is a proxy to infer the presence of micro-organisms.
Unfortunately the link between DNA reads and corresponding micro-organism is lost contrary to a single genome sequencing.
Reads aligned on a reference genome may originate from multiple contributors.
I had worked on a mixture model of distributions in order to bypass this information loss.
An overall idea of the approach developed is summarised in figure \ref{fig:mixture-model}.
From the CDS coverage density yields after reads alignment, the mixture model of distribution is meant to split this observed signal into contributions of defined distributions.
In our case the distributions used are based on the four densities in figure \ref{fig:distrib}. 

This approach comes down to multiple parameters estimation.
Providing proper parameters estimation, we plan to infer the presence or absence of a micro-organism based on estimated contributions.
Especially the contribution $\pi_0$ of the distribution modeling the closeness $0$ is likely to illustrate whether the reference genome is close to the strain in the ecosystem.


```{r mixture-model, engine="tikz",engine.opts=list(template="template-tikz2pdf.tex"),echo=FALSE, fig.align="center",fig.scap="Contributors genomes issues and mixture model principles.", fig.cap="Contributors genomes issues and mixture model principles.",cache=TRUE}
\input{figures/fig-mixture-model}
```


###Modeling exploration results

For parameter estimation, I rely on the canonical maximum likelihood method with the Expectation-Maximization algorithm [@dempster_maximum_1977].
I then applied this mixture model to simulated datasets and at the same time worked to counterbalance some model and method limitations.
Results of these joint explorations are described below.

####Mixture model limits and solutions



#####Number of classes
Four classes of distribution were relevant given the training dataset I designed.
However, they might be some cases where this number is either (1) too high, hence many superficial parameters needs to be estimated or (2) too low, thus the observed distribution will not be described properly.
To this end, I down-sized the model considering that four classes of closeness was the upper bound. I hence removed iteratively intermediate Gaussian distributions building a simpler model of two and three distributions.
The Gaussian distribution is meant to capture the information embodied in reads aligned from a close --at best exact-- strain.
The log-normal distribution intend to fit noise or distant genomes reads alignment.
I decided to use the BIC --_Bayesian Information Criteria_-- to choose between these models.
It provides strong penalties towards parameters numbers thus enabling parsimonious model selection.
The relevance of a single log-normal distribution test was examined, especially for one specific case with simulated data.
However, the presence of only one organism is highly unrealistic in metagenomics, hence a minimum of 2 distributions was settled.

#####Aligned reads number and start values
The EM algorithm --used for parameters estimation-- provides local optimum and hence is known to be sensitive to start values.
In addition, the mixture model was first designed for a fixed number of reads.
I decided to treat the algorithmic and methodologic limit --respectively the first and second mentioned above-- at the same time.
Therefore, I explored the influence of aligned reads number on the mixture model distributions.
The idea was to show the expected resolution loss between the closeness classes when the aligned reads number decreased.
In the meantime, it provides first grasps on model behaviour towards low-abundant micro-organisms --which are obviously yielding few reads.

```{r rho,fig.pos='h!',fig.align='center',fig.cap="Aligned reads number influence on one parameter: $\\rho$ or aligned CDS ratio. Each dot represents an alignment metric on a genome--the ratio of CDS with at least one read aligned-- function of the effective number of reads aligned. Data stem from the \\textit{Streptococcus} dataset. Reference genomes are classified on their closeness to the reference strain we choose throughout this study --see figure \\ref{fig:strep}. Non-linear regressions for each class are printed on plain lines. Each parameters --with respect to Michaelis-Menten formula-- yields significant estimation.",fig.scap="Aligned reads number influence on one parameter: $\\rho$ or aligned CDS ratio.",echo=FALSE,  cache=TRUE, warnings=FALSE,fig.height=3,fig.width=5}
tabRho<-read.csv("rhoData.csv")
ggplot(tabRho,aes(x=NbReadsObserved, y=1-Prop,color=as.factor(Prox))) +
geom_point() + scale_color_manual(values=c("#2aa2e0","#f47d20", "#8bac21", "#ed1a5c"),labels=c("0: 1 reference","1: 3 close strains","2: 3 sub-species","3: 3 close species"))+
geom_smooth(aes(group=as.factor(Prox)), method= "nls", formula="y~(Vm*x/(K+x))",method.args=list(
	start = c(Vm=0.7,K=800),
	lower=c(Vm=0.1,K=0.5), upper=c(Vm=1,K=20000),
	algorithm = "port"),
se=F,fullrange=TRUE)+theme_bw() + labs( title="Aligned CDS ratio evolution\nwith reads numbers",y=expression(paste("% aligned CDS: 1-", rho,sep="")), x="Number of reads aligned on CDS", color="Source genome class")
```
Reads number is an input parameter when crafting simulated datasets. However, it is different from the effective aligned reads number on the reference.
Actually only the latter is known with real datasets, so our predictions needs to rely on this effective number rather than the input parameter.
This information is provided on the x-axis of figure \ref{fig:rho}, where its variation is highlighted with the ratio of aligned CDS in the reference genome.
Despite stating the obvious, we see that the more close the genome is to the reference, the more reads effectively aligned.
Aligned CDS ratio shows a plateau for each class as long as reads number increase.
This reflects the fraction of CDS shared by the reference genome and the genome from which reads where sequenced.

The aforementioned tendency could be captured with non linear regression. The fit for each class was based on Michaelis-Menten formula and each parameters --such as $V_m$ or $K$ yields significant estimations.
For the parameters of mixture model pure distributions --gaussians and log-normal--, I observed a linear tendency between aligned reads number and the mean $\mu$ or the variance $\sigma^2$ --yet with different coefficient.

The strategy set up to circumvent start values issues will encompass both prior knowledge --in the form of regression models-- and a wider space parameter exploration --in the form of random sampling.
Hence, the following initialisation: (1) contributions $\pi_c$ are drawn from a uniform random law, (2) ratios $\rho_c$ are initialised with respect to the number of reads aligned based on non linear regression models above, and finally (3) distribution parameters --such as Gaussian and log-normal-- are initially set up both by predicting from linear regressions and by sampling in four times the prediction interval.
This approach yields several mixture models ranked with respect to the BIC criterion, hence selecting the most likely.

---

Several models are tested and the space of parameters is well explored. I had also written several R scripts to explore alternative models generated.
A report is automatically created giving an overview of model estimation. However, parameters estimations seems far from the primary aims of identifying micro-organisms. I then worked on decisions rules.


####Decision rules concerning the mixture model output

#####Composition dataset
In order to bridge the gap from contribution estimations to a boolean vector of presence / absence of micro-organisms, I explored decisions rules from the mixture model.
I designed a dataset termed _composition dataset_ different from the training dataset with _Streptococcus_ but with a similar approach.
This dataset actually consists of 3 subdatasets of growing complexity --from 1 the simpler to 3 the more complex.

The first dataset encompass reads from a selected reference genome -- here _Lactococcus lactis subsp. cremoris_ A76-- mixed with reads from a _noise_ genome. This distant genome needs to be close enough from the reference to provide reads that can be aligned and distant enough from the reference to be able to distinguish them.
I choosed _Lactococcus raffinolactis_ JIM2957 genome to be the _noise_ genome.
I expected this data to illustrate strain identification when the reference genome is available. 

The second dataset encompass reads stem from the reference genome, the noise genome and an addtional close strain --_Lactococcus lactis subsp. cremoris_ MG1363 from closeness class $1$.
Similarly the third dataset --noted $3$-- comprise the latter dataset with a supplementary genome to stem the reads from. The sub species _Lactococcus lactis subsp. lactis_ Il1403 is the one added.

From these composition of genomes, I simulated two shotgun metagenomics sequencing at two different coverages: high coverage --$1 \times$-- and low coverage --$0.1 \times$. They are respectively noted HC and LC.
The 3 datasets at two coverages are hence aligned to 10 reference genomes distributed like the _Streptococcus_ earlier: 1 reference genome, 3 close strains, 3 sub-species and 3 close species or distant genomes.

#####Decision rules

After reads alignement on each genome, parameters are estimated for all differents models presented above and a model is selected thanks to the BIC criterion.
The mixture component $c=0$ and its estimated parameters will be used for decisions rules.
We expect this component to capture the information of the closest strain. Hence if the exact strain is present, the signal will be strong to support this information. 
At first, I outlined two rules to decide whether the exact reference genome had contribute to the alignment observed.
Closeness class $0$ contribution to the entire mixture model needed to be important --say $\pi_0 > 0.3$-- and the number of CDS uncovered should be negligle for a fair number of reads aligned --say $\rho_c$ < 0.01.
These thresholds stem from prior design and local observations. As the approach was still developed, I settled for the previous values and assess them onto a dataset in order to refine them if needed.


####Mixture model results

#####Preliminary results on simulated datasets



```{r composition,fig.pos='h!',fig.align='center',cache=TRUE,warnings=FALSE,echo=FALSE, fig.scap="Composition dataset outcomes after mixture model estimation.", fig.cap="Composition dataset outcomes after mixture model estimation. On the y-axis are the 10 reference genomes onto which reads alignment was done. Some of them were chosen to simulate reads from their genome sequence. The x-axis consists of the two coverages used in the simulation. Labeled facets separate results for each dataset.  Results consists in a combination of truth vectors: one stating the presence/absence according to simulated datasets and the other one yields by the outcome of decision rules mentioned above. Symbols highlights the four possible values of the combination. "}
allG<-readRDS("lactoTabEM.RDS")
ggplot(allG[which( allG$Component == 0 & allG$JDD_Index %in% c("1","2","3")),], aes(color=interaction(JDD_Presence,Presence),y=Genome,x=JDD_Coverage, shape=interaction(JDD_Presence,Presence)))+ geom_point(size=3) + facet_wrap(~JDD_Index) + theme_bw() +theme(legend.position="bottom")+scale_y_discrete(labels=function(x) gsub("Lactococcus_","L.",x)) +scale_shape_manual(label=c("True Negative","False Negative","False Positive","True Positive"),values=c(4,2,5,1),limits=c("FALSE.FALSE","TRUE.FALSE","FALSE.TRUE","TRUE.TRUE"))+scale_color_manual(label=c("True Negative","False Negative","False Positive","True Positive"),values=c('#000000','#e41a1c','#4daf4a','#000000'),limits=c("FALSE.FALSE","TRUE.FALSE","FALSE.TRUE","TRUE.TRUE"))+labs(title="Reference Genome Presence/Absence\n Composition dataset", shape="Result",color="Result",x="Coverage")+guides(shape=guide_legend(ncol = 2),color=guide_legend(ncol = 2))
```

The first observation we can state from figure \ref{fig:composition} is that in low coverage datasets no genome was considered present with the given thresholds. This high rate of false negative occurs despite an important ratio of covered CDS --data not shown.
Exact strains were retrieved only in high coverage experiment. However, false positives were also retrieved. They matches to close strains which is not specifically a wrong outcome.

The effect of threshold was assessed by encapsulate this plot into a Shiny application in order to interactively explore decision rules.
However, no suitable pattern emerge from this exploration and there was no interesting threshold combination.

#####Others datasets
I then design a _verification_ dataset in order to assess the following questions: (1) How to determine a genome absence in a dataset despite an important number of reads aligned? and (2) Could we identify the exact strain if few reads from this strain genome are aligned in addition to this important noise signal?
It was constructed with the reference genome mentioned above with coverage of $0.01 - 0.1\times$ to  $10-100\times$.
The results are not show here but the conclusion is similar to earlier, this mixture model yields proper results only when the coverage is important --better than $0.1\times$. This lower bound in disappointing as this coverage values is considered high in metagenomics.

I also ran Results on the Mock Community.
The Mock Community is the real sequencing experiment conducted on a known composition microbe cocktail. This metagenomics dataset was supposed to be a trivial test but unfortunately it yields too many false negatives again.


#####A more relevant mock community? 
I had planned to test this model on a mock dataset designed by Dugat-Bony et al [see -@dugat-bony_overview_2015]. 
This microbial community dataset is relevant to our projects because it is associated with cheese ecosystems and it has been sequenced with SOLiD technology.
Time constraints prevented this dataset to be tested as well as aforementioned negative results on the Mock Community. 
In spite of an almost perfect match of this dataset to the scope of our project, I did not used the mixture model approach on it yet.



##Integration

###Integration with the Food-Microbiome Transfert database

In the scope of the Food-Microbiome Transfert project, a metadata-extended genomics database has been build thanks to a bioinformatician engineer recently in the team. 
The interaction between our metagenomics analysis tool GeDI and this database was first described in figure \ref{fig:actors}.
Actually, GeDI fetch genomes and annotations information from flat files in directories. This system is totally relevant and based on standard files such as FASTA and Genbank files.
However, in the scope of the Food-Microbiome project, we planned to fetch these information directly from the database.

###Integration with the Galaxy platform

A tool is relevant only if is it used. In order to promote the use of our metagenomics analysis tool, we worked to integrate it to the Galaxy platform.
The Galaxy platform is a web-based tool providing (1) easy access to command line tools and (2) data history and workflow management.
It is hosted on the same location of the migale server.
A bioinformatician engineer in the team attended a training to integrate new tools into the Galaxy platform.
Our tool’s integration was his exercise throughout this course. 
This training was an opportunity for me to craft small datasets and documents to provide him material for tests.
Remarks and conclusions after this formation were taken into account to properly adapt our tool to some requirements, filetypes for instance.

###Integration with other tools and software performance

In addition to scientific improvements covered above, I assess our tool performance and modify it in order to enhance them.
I was mainly working on converting in-house files generated by GeDI into standard filetype in bioinformatics --such as BAM and GFF. 
We expected a decrease in file size due to the compression of alignment data with BAM files.
Likewise, we expected an increase in software speed due to the transition to highly efficient tools such as samtools instead of home-made scripts.
These expectations were met with order of magnitude for speed and disk space.

Standard files ease the exporting to other tools. For example, industrial partners and academics will be able to export metagenomics reads alignment on their strain of interest, and visualize data with Tablet or process it for parallel downstream analyses. 

## Application

* to simulated datasets
* to cheese metagenomics analysis : 6 samples
